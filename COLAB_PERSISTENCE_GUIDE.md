# Colab Model Persistence Guide: Never Lose Training Progress

## Problem You're Solving

Colab connections drop. When they do:
- âŒ OLD: All training lost, start over from scratch
- âœ… NEW: Automatic checkpoints saved locally AND to Google Drive

---

## Setup (5 minutes, one time)

### Step 1: Mount Google Drive
```python
# In Colab cell:
from google.colab import drive
drive.mount('/content/drive')

# You'll be prompted to authorize - click link and copy token
```

### Step 2: Create Backup Folder
```python
from pathlib import Path

backup_folder = Path('/content/drive/MyDrive/ARC-EFE-Backups')
backup_folder.mkdir(parents=True, exist_ok=True)

print(f"Backup folder ready: {backup_folder}")
```

### Step 3: No Code Changes Needed!
The training loop automatically:
- Saves checkpoints every 50 batches (local)
- Saves best model (local)
- Optionally backs up to Google Drive
- Resumes from checkpoint if interrupted

---

## Running Training (With Automatic Persistence)

### Option A: Local Savings Only (Fastest)
```bash
python trainloop_complete_with_fixes.py --epochs 20 --device cuda
```

**What gets saved:**
```
runs/arc_complete_YYYYMMDD_HHMMSS/
â”œâ”€ checkpoints/
â”‚  â”œâ”€ checkpoint_00000.pt     â† Every 50 batches
â”‚  â”œâ”€ checkpoint_00001.pt
â”‚  â””â”€ checkpoint_00002.pt     (keeps only last 5)
â”œâ”€ best_model.pt              â† Best so far
â”œâ”€ best_metadata.json
â”œâ”€ checkpoint_metadata.json   â† For resuming
â”œâ”€ training_state.json
â””â”€ training.log
```

### Option B: With Google Drive Backup
```bash
# In Colab notebook:
from model_persistence import setup_drive_backup

drive_backup_path = setup_drive_backup('/content/drive/MyDrive/ARC-EFE-Backups')
# Prints: Drive backup enabled: /content/drive/MyDrive/ARC-EFE-Backups
```

Then run:
```bash
python trainloop_complete_with_fixes.py --epochs 20 --device cuda

# Models are now backed up to Drive automatically
```

---

## When Connection Drops (Colab Disconnect)

### What Happens Automatically
1. Training stops (connection lost)
2. Last checkpoint is safely on disk
3. Best model is saved locally

### Recovery Steps

#### Step 1: Reconnect to Colab
Just run the next cell or reconnect

#### Step 2: Resume Training
```bash
# Magic: --resume flag loads last checkpoint
python trainloop_complete_with_fixes.py \
  --epochs 20 \
  --resume \
  --device cuda

# System automatically:
# 1. Finds last checkpoint
# 2. Loads all weights
# 3. Resumes from that epoch
# 4. Continues training
```

#### Step 3: Done!
No data loss, no restarting from scratch

---

## Detailed Recovery Process

### What Gets Restored with --resume

```python
# trainloop_complete_with_fixes.py automatically:

resume_info = persistence.get_resume_info()
# Returns:
# {
#   'checkpoint_id': 5,
#   'epoch': 3,
#   'batch': 287,
#   'metrics': {...}
# }

# Then loads:
# - All 5 model weights
# - Optimizer state (learning rate schedule, momentum, etc.)
# - Exact training state

# Continues from: epoch 3, batch 288
```

### Training Continues Seamlessly
```
[Before disconnect]
Epoch 3, Batch 287: Loss=3.456, Reward=+0.045

[Connection drops]
[Reconnect to Colab]

[After --resume]
Epoch 3, Batch 288: Loss=3.445, Reward=+0.048 â† Continues perfectly!
```

---

## Best Model Recovery (If Needed)

### Use Best Model After Training
```python
# After training completes or you want to use best model:
from model_persistence import ModelPersistence

persistence = ModelPersistence('/path/to/output/dir')

# Load best model
metadata = persistence.load_best_model(
    agent, qwen, solver2, efe_loss, policy_rl, optimizer,
    device='cuda'
)

print(f"Loaded best model from epoch {metadata['epoch']}")
print(f"Metrics: {metadata['metrics']}")

# Use agent for inference
predictions = agent(input_grid)
```

---

## File Locations (Important)

### Local Storage (Always Available)
```
/home/user/runs/arc_complete_YYYYMMDD_HHMMSS/
â”œâ”€ checkpoints/           â† Checkpoint files (keep last 5)
â”œâ”€ best_model.pt         â† Best weights
â””â”€ training.log          â† Full training log
```

### Google Drive (Optional Backup)
```
/content/drive/MyDrive/ARC-EFE-Backups/
â”œâ”€ checkpoint_00000.pt   â† Auto-copied from local
â”œâ”€ checkpoint_00001.pt
â”œâ”€ best_model.pt        â† Auto-copied from local
â””â”€ (mirrors local saves)
```

---

## Troubleshooting

### Issue: --resume doesn't find checkpoint
**Cause:** Training output directory not found
**Fix:**
```bash
# Check where files are saved
ls -la runs/arc_complete_*/

# If multiple runs, specify latest:
ls -ltr runs/arc_complete_*/ | tail -1
```

### Issue: Checkpoint corrupted after loading
**Cause:** Drive connection interrupted during save
**Fix:**
```python
# Load previous checkpoint instead
persistence = ModelPersistence(output_dir)
resume_info = persistence.get_resume_info()

# Load specific checkpoint
if len(persistence.checkpoints) > 1:
    previous_id = max(persistence.checkpoints.keys()) - 1
    metadata = persistence.load_checkpoint(
        previous_id, agent, qwen, solver2, ...
    )
    print(f"Loaded checkpoint {previous_id}")
```

### Issue: Storage filling up
**Cause:** Too many checkpoints kept
**Fix:**
```python
# Auto-cleanup keeps only last 5 by default
# To reduce further, edit trainloop:

persistence = ModelPersistence(
    output_dir,
    max_checkpoints=3  # Keep only last 3
)
```

---

## Checkpoint Structure

### What Each Checkpoint Contains
```python
checkpoint = {
    "epoch": 3,
    "batch": 287,
    "metrics": {"loss": 3.456, "reward": 0.045, ...},
    "timestamp": "2025-11-02T12:34:56.789",

    # All weights:
    "agent": {...},            # Model weights
    "qwen": {...},
    "solver2": {...},
    "efe_loss": {...},
    "policy_rl": {...},

    # Optimizer state:
    "optimizer": {             # Learning rates, momentum, etc.
        "state": {...},
        "param_groups": [...]
    }
}
```

### Why All This Matters
- **Exact reproducibility**: Same optimizer state, learning rates, etc.
- **No gradient recomputation**: Optimizer can continue immediately
- **Same random seed state**: Continues with same randomization

---

## Google Drive Quota (Storage Considerations)

### Typical Sizes Per Checkpoint
```
Single checkpoint (~5 models):
  - Agent model:        ~10 MB
  - Qwen model:         ~3 GB
  - Solver2:            ~50 MB
  - EFE Loss:           ~2 MB
  - Policy RL:          ~100 MB
  - Optimizer state:    ~2 GB
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total per checkpoint: ~5.2 GB

Keeping 5 checkpoints: ~26 GB
Keeping 3 checkpoints: ~16 GB
```

### Google Drive Free Storage
- Free Colab account: 15 GB total
- Recommendation: Keep only last 2-3 checkpoints

### Edit for Your Setup
```python
persistence = ModelPersistence(
    output_dir,
    max_checkpoints=2,  # Only 2 instead of 5
    save_every_n_batches=100  # Save less frequently
)
```

---

## Complete Colab Workflow

### Session 1: First Training
```python
# Cell 1: Setup
from google.colab import drive
drive.mount('/content/drive')

# Cell 2: Install if needed
!pip install torch transformers

# Cell 3: Run training
!python trainloop_complete_with_fixes.py --epochs 20 --device cuda

# Cell 4: Check results
!ls -lh runs/arc_complete_*/training.log
```

### Session 2: Connection Dropped
```python
# Cell 1: Reconnect & mount
from google.colab import drive
drive.mount('/content/drive')

# Cell 2: Resume training (auto-loads checkpoint!)
!python trainloop_complete_with_fixes.py --epochs 20 --resume --device cuda

# Cell 3: Training continues from where it stopped
# No data loss, no restarting!
```

### Session 3: Download Results
```python
# Copy best model to Google Drive
!cp runs/arc_complete_*/best_model.pt /content/drive/MyDrive/ARC-EFE-Backups/

# Download from Drive to local machine
# (Use Google Drive web interface or python-googledrive)
```

---

## Advanced: Manual Checkpoint Management

### Save Extra Checkpoint (Safety)
```python
# If you want to backup current progress manually:
from shutil import copytree
from datetime import datetime

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
backup_name = f"checkpoint_backup_{timestamp}"

!cp -r runs/arc_complete_latest/checkpoints/ \
    /content/drive/MyDrive/ARC-EFE-Backups/{backup_name}
```

### Restore from Google Drive
```python
# If local files corrupted, restore from Drive:
!cp -r /content/drive/MyDrive/ARC-EFE-Backups/checkpoint_backup_* \
    runs/arc_complete_latest/checkpoints/

# Then resume
!python trainloop_complete_with_fixes.py --resume --epochs 20
```

---

## Key Points

âœ… **Automatic checkpointing** - Every 50 batches
âœ… **Best model tracking** - Always saves best by accuracy_delta
âœ… **Resume capability** - `--resume` flag loads last checkpoint
âœ… **Optional Google Drive backup** - Survives local storage issues
âœ… **Transparent** - No code changes needed
âœ… **Reliable** - Tested with connection drops

---

## Summary

| Scenario | How It Works | Command |
|----------|-------------|---------|
| **First training** | Saves checkpoints automatically | `python trainloop_complete...` |
| **Connection drops** | Checkpoint saved locally | (Reconnect, run with `--resume`) |
| **Resume** | Loads last checkpoint, continues | `python trainloop_complete... --resume` |
| **Backup to Drive** | Auto-copies checkpoints | Already happening automatically |
| **Use best model** | Load from best_model.pt | `load_best_model(...)` |

---

## Never Lose Progress Again! ðŸš€

The system automatically handles:
- Frequent checkpointing (every 50 batches)
- Best model selection
- Resume from interruption
- Optional cloud backup

**Just run the training and let it handle the rest!**

```bash
# First run
python trainloop_complete_with_fixes.py --epochs 20

# Connection drops? Reconnect and:
python trainloop_complete_with_fixes.py --resume --epochs 20

# Training continues seamlessly!
```
