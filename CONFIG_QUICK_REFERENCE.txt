â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    TRAINING PIPELINE - FILES AND CONFIG QUICK REFERENCE           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


ENTRY POINTS (Choose One)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. run_main.py
   â””â”€ Full pipeline (train + test + report)
   â””â”€ Entry: main()
   â””â”€ Output: runs/arc_full_run_YYYYMMDD_HHMMSS/

2. run_gpu_finetuned.py
   â””â”€ GPU-optimized training
   â””â”€ Entry: main() in trainloop_gpu_finetuned.py
   â””â”€ Output: runs/arc_gpu_finetuned_YYYYMMDD_HHMMSS/

3. run_cpu_simple.py
   â””â”€ CPU-only training
   â””â”€ Entry: main() in trainloop.py
   â””â”€ Output: runs/arc_cpu_YYYYMMDD_HHMMSS/


CORE TRAINING FILES (7 Files)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Execution & Orchestration:
  â”œâ”€ run_main.py                    PipelineConfig
  â”œâ”€ run_gpu_finetuned.py           Wrapper for trainloop_gpu_finetuned
  â”œâ”€ run_cpu_simple.py              Wrapper for trainloop
  â”œâ”€ train_sequence.py              Main training loop (TrainingConfig, TrainingLogger)
  â”œâ”€ trainloop.py                   Single epoch training
  â”œâ”€ trainloop_gpu_finetuned.py     GPU training + Qwen fine-tuning (MAIN LOOP)
  â””â”€ test_sequence.py               Testing/evaluation


MODEL ARCHITECTURE FILES (6 Files)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Core Models:
  â”œâ”€ loss_function.py               EFELoss, EFELossConfig, ARCPromptGuidedAgent
  â”œâ”€ qwen_hybrid_prompt.py          QwenHybridPrompt, QwenCfg
  â”œâ”€ revthink_orchestrator.py       RevThinkOrchestrator, RevThinkCfg
  â”œâ”€ tta.py                         TestTimeAdaptationSystem, SurpriseBasedMemory
  â”œâ”€ solver1.py                     ContextualMemoryBank, ContextualSolver
  â””â”€ solver2.py                     PermanentMemoryBank, PermanentSolver


DATA & FEATURE FILES (4 Files)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Data Processing:
  â”œâ”€ dataset_arc.py                 ARCDataset (loads JSON, splits data)
  â”œâ”€ feature_extraction.py          extract_transformation_features() [15-dim]
  â”œâ”€ feature_registry.py            FeatureRegistry (operator management)
  â””â”€ configs/operators.yaml         Feature operator configuration


ALTERNATIVE LOSS FILES (3 Files - Optional)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â”œâ”€ grid_accuracy_loss.py          Alternative: Grid accuracy focused
  â”œâ”€ grid_transformation_loss.py    Alternative: Transformation focused
  â””â”€ priority_efe_loss.py           Alternative: Priority-based


UTILITY FILES (1 File)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â””â”€ measure_throughput.py          Performance benchmarking


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONFIGURATION LOCATIONS (WHERE TO CHANGE SETTINGS)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

HARDCODED CONSTANTS (6 Parameters)
  File: train_sequence.py (Lines 23-29)
  â””â”€ TTA_EVAL_INTERVAL = 50
  â””â”€ CHECKPOINT_INTERVAL = 100
  â””â”€ GRAD_CLIP_NORM = 1.0
  â””â”€ LEARNING_RATE = 1e-3
  â””â”€ EPOCHS = 5
  â””â”€ BATCH_SIZE = 1


FUNCTION DEFAULTS - trainloop_gpu_finetuned.py (14 Parameters)
  Location: Lines 505-509, main() function

  Training:
    â”œâ”€ epochs=10
    â”œâ”€ agent_lr=1e-5
    â”œâ”€ qwen_lr=None
    â”œâ”€ weight_decay=1e-6
    â”œâ”€ grad_accum_steps=1
    â”œâ”€ grad_clip=1.0
    â”œâ”€ warmup_steps=100

  Checkpointing:
    â”œâ”€ save_frequency=1

  Evaluation:
    â”œâ”€ max_batches_per_epoch=None
    â”œâ”€ val_frequency=1
    â””â”€ skip_test=False

  System:
    â”œâ”€ device="cuda"
    â”œâ”€ seed=42
    â””â”€ model_name=None

  ğŸ†• Model Control:
    â””â”€ freeze_qwen=True  â† NEW: Set to False to train Qwen!


LOSS WEIGHTS - EFELossConfig (11 Parameters)
  File: loss_function.py (Lines 25-88)
  Class: @dataclass EFELossConfig

  Lambda Weights:
    â”œâ”€ lambda_risk = 1.0              Risk/preference matching
    â”œâ”€ lambda_amb = 0.0               Ambiguity reduction
    â”œâ”€ lambda_step = 0.1              Step penalty
    â”œâ”€ lambda_cons = 1.0              Consistency (GRID MATCHING PRIMARY)
    â”œâ”€ lambda_bi = 0.5                Bidirectional agreement
    â”œâ”€ lambda_z = 0.2                 Z-learning anchoring
    â”œâ”€ lambda_prompt = 0.3            Prompt influence
    â”œâ”€ lambda_grid_norm = 0.1         Grid size fairness
    â””â”€ lambda_reversibility = 0.4     Reversibility check (NEW)

  Grid Params:
    â”œâ”€ max_grid_size = 30
    â”œâ”€ num_colors = 10
    â””â”€ prompt_dim = 256

  Advanced:
    â”œâ”€ inference_first_threshold = 0.7
    â””â”€ ema_decay = 0.99

  ğŸ†• Pre-configured Profiles:
    â”œâ”€ EFELossConfig.aggressive_grid_matching()
    â”œâ”€ EFELossConfig.reversibility_focus()
    â””â”€ EFELossConfig.balanced()


QWEN CONFIGURATION - QwenCfg (5 Parameters)
  File: qwen_hybrid_prompt.py (Lines 224-240)
  Class: QwenCfg

    â”œâ”€ model_name = "Qwen/Qwen2.5-1.8B"
    â”œâ”€ dtype = "float16"
    â”œâ”€ max_new_tokens = 96
    â”œâ”€ temperature = 0.0              (Deterministic)
    â””â”€ top_p = 0.9


REVTHINK CONFIGURATION - RevThinkCfg (6 Parameters)
  File: revthink_orchestrator.py (Lines 7-13)
  Class: @dataclass RevThinkCfg

    â”œâ”€ tau = 0.45                    Revision trigger threshold
    â”œâ”€ alpha = 2.0                   Gate sharpness
    â”œâ”€ beta = 0.3                    Gate bias
    â”œâ”€ gamma = 0.5                   Lambda prompt boost (prompts updated with gamma)
    â”œâ”€ eta = 0.2                     Z-anchoring blend
    â””â”€ mask_weight = 0.5


PIPELINE CONFIGURATION - PipelineConfig (9 Parameters)
  File: run_main.py (Lines 18-48)
  Class: PipelineConfig

  Training:
    â”œâ”€ train_epochs = 5
    â”œâ”€ train_batch_size = 1
    â”œâ”€ train_lr = 1e-3
    â”œâ”€ train_grad_clip = 1.0
    â”œâ”€ train_tta_eval_interval = 50
    â””â”€ train_checkpoint_interval = 100

  Testing:
    â”œâ”€ test_on_train_split = False
    â”œâ”€ test_on_test_split = True
    â”œâ”€ use_tta = True
    â””â”€ tta_steps = 5

  Reporting:
    â””â”€ generate_report = True


TTA CONFIGURATION (4 Parameters)
  File: tta.py (Lines 33-48)
  Class: TestTimeAdaptationSystem.__init__()

    â”œâ”€ memory_size = 1000
    â”œâ”€ surprise_threshold = 0.65
    â”œâ”€ adaptation_steps = 5
    â””â”€ adaptation_lr = 1e-3


SOLVER1 CONFIGURATION (3 Parameters)
  File: solver1.py (Lines 28-40)
  Class: ContextualMemoryBank.__init__()

    â”œâ”€ context_size = 50
    â”œâ”€ surprise_threshold = 0.2
    â””â”€ temporal_decay = 0.9


SOLVER2 CONFIGURATION (2 Parameters)
  File: solver2.py (Lines 161-172)
  Class: PermanentMemoryBank.__init__()

    â”œâ”€ max_memories = 10000
    â””â”€ DBSCAN eps = 0.3  (in clusterer definition)


ENVIRONMENT VARIABLE (1 Parameter)
  Location: trainloop_gpu_finetuned.py (Line 546)

    â””â”€ ARC_DATA_DIR       (Path to data, defaults to ".")


FEATURE CONFIG (External YAML)
  File: configs/operators.yaml
  â””â”€ Custom feature operators


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

HOW TO CHANGE CONFIGURATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

METHOD 1: Command Line Arguments (EASIEST)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  python run_gpu_finetuned.py \
    --epochs 20 \
    --agent_lr 5e-5 \
    --freeze_qwen False \
    --weight_decay 1e-5


METHOD 2: Use Config Profiles (RECOMMENDED for Loss)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  from loss_function import EFELossConfig, EFELoss

  # Aggressive grid matching (4Ã— boost on consistency)
  cfg = EFELossConfig.aggressive_grid_matching()
  efe_loss = EFELoss(**cfg.to_dict())

  # Reversibility focus (maximize invertibility check)
  cfg = EFELossConfig.reversibility_focus()
  efe_loss = EFELoss(**cfg.to_dict())

  # Balanced (default)
  cfg = EFELossConfig.balanced()
  efe_loss = EFELoss(**cfg.to_dict())


METHOD 3: Edit Function Defaults (Permanent Change)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  File: trainloop_gpu_finetuned.py (Lines 505-509)

  Change:
    def main(epochs=10, agent_lr=1e-5, freeze_qwen=True, ...):
  To:
    def main(epochs=20, agent_lr=5e-5, freeze_qwen=False, ...):


METHOD 4: Direct Class Instantiation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  from loss_function import EFELossConfig, EFELoss

  cfg = EFELossConfig()
  cfg.lambda_cons = 2.0      # Double grid matching
  cfg.lambda_bi = 0.3        # Reduce bidirectional
  cfg.freeze_qwen = False    # Train Qwen

  efe_loss = EFELoss(**cfg.to_dict())


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TOTAL CONFIGURATION SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Total Files Used in Training:         21
  â”œâ”€ Core execution:                  7
  â”œâ”€ Model architecture:              6
  â”œâ”€ Data & features:                 4
  â””â”€ Utilities/alternatives:          4

Total Configurable Parameters:        50+
  â”œâ”€ Loss weights:                    11
  â”œâ”€ Training hyperparameters:        14
  â”œâ”€ Model configurations:            13
  â”œâ”€ TTA/Solver settings:             9
  â””â”€ Environment/system:              3

Configuration Sources:                6
  â”œâ”€ Hardcoded constants
  â”œâ”€ Function defaults
  â”œâ”€ Dataclass defaults
  â”œâ”€ Command line arguments
  â”œâ”€ Environment variables
  â””â”€ External YAML file


KEY PARAMETERS TO ADJUST
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For Better Grid Matching:
  âœ“ Use: EFELossConfig.aggressive_grid_matching()
  âœ“ Or: lambda_cons = 2.0 (double from 1.0)

For Better Reversibility:
  âœ“ Use: EFELossConfig.reversibility_focus()
  âœ“ Or: lambda_reversibility = 0.8

To Train Qwen:
  âœ“ Set: freeze_qwen = False
  âœ“ Or: python run_gpu_finetuned.py --freeze_qwen False

To Monitor Training:
  âœ“ Check: runs/arc_gpu_finetuned_YYYYMMDD_HHMMSS/
  âœ“ Files: train_log.jsonl, metrics.jsonl, metrics_plot.png

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš€ READY TO TRAIN WITH FULL CONFIGURATION CONTROL!
