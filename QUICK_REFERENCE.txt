================================================================================
COMPLETE TRAINING FIXES - QUICK REFERENCE
================================================================================

THREE CRITICAL ISSUES FIXED:

[1] TENSOR MISMATCH (Earlier)
    File: fixed.py:325
    Before: out.float()
    After:  out.long()
    ✓ FIXED

[2] QWEN GRADIENTS BLOCKED (Major)
    File: policy_refined.py:144
    Before: @torch.no_grad()
            def apply(self, ...):
    After:  def apply(self, ...):  # decorator removed
    ✓ FIXED

[3] BACKWARD GRAPH CONFLICT (Latest)
    File: fixed.py:277 → 362
    Before: policy_rl.update() BEFORE loss.backward()
    After:  policy_rl.update() AFTER scaler.step()
    ✓ FIXED

================================================================================
FIVE IMPROVEMENTS APPLIED:

[1] Qwen Gradients      → Enabled (@torch.no_grad() removed)
[2] Hard-Cell Masking   → (0.5 + 0.5 * mask_ratio)
[3] Reward Scaling      → reward * 5.0
[4] Qwen Overhead       → Prompt caching every 10 batches
[5] Curriculum Weight   → 0.6 (epoch<1) or 0.3 (epoch≥1)

================================================================================
GRADIENT FLOW PATH:

qwen()
  ↓
qwen_prompt (with requires_grad=True)
  ↓
refine_prompt() [FIXED: no @torch.no_grad()]
  ↓
refined_prompt
  ↓
efe_loss()
  ↓
loss.backward() [FIXED: clean graph]
  ↓
Qwen parameters updated ✅
  ↓
policy_rl.update() [FIXED: after backward]

================================================================================
EXPECTED FIRST RUN BEHAVIOR:

Batch 0:    Qwen_grad: 0.00e+00 (computing)
Batch 10:   Qwen_grad: 1.23e-05 ← SHOULD BE NONZERO
Batch 50:   Qwen_grad: 5.67e-05 ← STABLE
Loss:       Decreasing trend
Mask_ratio: Declining (1.0 → 0.8 → 0.6)
Reward:     Trending positive

================================================================================
KEY MONITORING:

✅ HEALTHY:
   - Qwen_grad: nonzero (1e-6 to 1e-4)
   - Loss: downward trend
   - Mask_ratio: declining
   - No NaN/Inf values

❌ RED FLAGS:
   - Qwen_grad: 0.00e+00 for 100+ batches
   - Loss: flat or increasing
   - Loss: NaN/Inf
   - Errors in backward pass

================================================================================
RUN COMMAND:

python fixed.py \
  --epochs 10 \
  --agent_lr 1e-5 \
  --qwen_lr 5e-5 \
  --device cuda \
  --max_batches 500

================================================================================
STATUS: [FULLY FIXED - READY TO TRAIN] ✅
================================================================================
